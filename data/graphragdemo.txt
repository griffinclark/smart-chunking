Graph LLM, Demo Outline
Text2Cypher
Graph RAG
KG Building
📼 Recording of this workshop: https://youtu.be/hb8uT-VBEwQ

🚀 URL of this notebook: https://bit.ly/graph-rag-workshop

Text2GraphQuery/ Text2Cypher
What is Cypher?
// Pattern of vertex/node
(foo:tagA)

// Pattern of edge/relationship
()-[e:type_x]->()

// Pattern Matching
MATCH p=(foo:tagA{name: "Jerry"})-[:type_x]->()
RETURN p LIMIT 5

// Pattern Matching with WHERE clause, NebulaGraph Cypher
MATCH (e1:entity)-[r:relationship]->(e2:entity)
WHERE id(e1) == 'James Gunn'
RETURN e2.entity.name
What and Why Text2Cypher with LLM
TextCypher with LLM

Text2Cypher

Values

The straightforward/low-hanging fruit approach to impl. Cognitive intelligence app against existing KG.
Decouple Graph Query expertise to access KG
Crash barrier to land graph-enabled services
Why with LLM

# Before, an example of KG QA system
┌─────────────┬───────────────────────────────────┐
│      Speech │  Frontend                         │
│  ┌──────────▼──────────┐ Siwi, /ˈsɪwi/          │
│  │ Web_Speech_API      │ A PoC of Dialog System │
│  │ Vue.JS              │ With Graph Database    │
│  │                     │ Backed Knowledge Graph │
│  └──────────┬──────────┘                        │
│             │  Sentence  Backend                │
│┌────────────┼────────────────────────────┐      │
││ ┌──────────▼──────────┐                 │      │
││ │ Web API, Flask      │ ./app/          │      │
││ └──────────┬──────────┘                 │      │
││            │  Sentence  ./bot/          │      │
││ ┌──────────▼──────────┐                 │      │
││ │ Intent Matching,    │ ./bot/classifier│      │
││ │ Symentic Processing │                 │      │
││ └──────────┬──────────┘                 │      │
││            │  Intent, Enties            │      │
││ ┌──────────▼──────────┐                 │      │
││ │ Intent Actor        │ ./bot/actions   │      │
│└─┴──────────┬──────────┴─────────────────┘      │
│             │  Graph Query                      │
│  ┌──────────▼──────────┐                        │
│  │ Graph Database      │  NebulaGraph           │
│  └─────────────────────┘                        │
└─────────────────────────────────────────────────┘
# With LLM(Fine-tuned or vanilla model), Prompt is all we need.

You are a NebulaGraph Cypher Export, please compose a query based on the given Graph Schema and question:
---
{schema}
---
Question:
---
{question}
---
Please start!
# With Llama Index, 3 lines of code
nl2kg_query_engine = KnowledgeGraphQueryEngine(
    storage_context=storage_context,
    service_context=service_context,
)
# Get Answer
response = nl2kg_query_engine.query(
    "Tell me about Peter Quill?",
)
# Or, only Compose query
graph_query = nl2kg_query_engine.generate_query(
    "Tell me about Peter Quill?",
)
Refs

https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_query_engine.html
https://siwei.io/en/llm-text-to-nebulagraph-query/
https://siwei.io/en/demos/text2cypher/
Graph RAG
Graph RAG with LLM

Graph RAG

Query time workflow:

Get Key Entities/Relationships related to task
LLM or NLP to extract from task string
Expand synonyms
Get SubGraphs
Exact matching of "Starting Point"
Optionally Embedding based
Generate answer based on SubGraphs
Could be combined with other RAG
If KG was built with LlamaIndex, metadata could be leveraged
Values

KG is __ of Knowledge:
Refined and Concise Form
Fine-grained Segmentation
Interconnected-structured nature
Knowledge in (existing) KG is Accurate
Query towards KG is Stable yet Deterministic
Reasoning/Info. in KG persist domain knowledge
Refs:

https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html
https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html
https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html
https://siwei.io/talks/graph-rag-with-jerry/
https://www.youtube.com/watch?v=bPoNCkjDmco
KG Building
KG Building

Value

Game-changer for ROI on adaptation of Graph
NLP Competence and efforts
Complex Pipelines
Those "nice to have" graphs can now be enabled by Graph at a small cost
Refs

https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.html#instantiate-gptnebulagraph-kg-indexes
https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_query_engine.html
https://colab.research.google.com/drive/1G6pcR0pXvSkdMQlAK_P-IrYgo-_staxd?usp=sharing
https://siwei.io/en/demos/text2cypher/
https://siwei.io/demo-dumps/kg-llm/KG_Building.ipynb
https://siwei.io/demo-dumps/kg-llm/KG_Building.html
How
with LlamaIndex and NebulaGraph

Concepts
REF: https://gpt-index.readthedocs.io/en/stable/getting_started/concepts.html

RAG
Retrieval Augmented Generation:



Indexing Stage


Data Connectors(LlamaHub)

Documents

Nodes(Chunk)
Index

VectorIndex
KnowledgeGraphIndex, create KG from data, Graph RAG
SQLIndex
Querying Stage
Query Engine/Chat Engine Agent(input text, output answer)
KnowledgeGraphQueryEngine, Text2Cypher Query engine
Retriever(input text, output nodes)
KnowledgeGraphRAGRetriever, for existing KG wired as Graph RAG
Node Postprocessor(Reranking, filterring nodes)
Response Synthesizer(input nodes, output answer)


Context
REF:

https://gpt-index.readthedocs.io/en/stable/core_modules/supporting_modules/service_context.html
https://gpt-index.readthedocs.io/en/stable/api_reference/storage.html
Service context

LLM
Embedding Model
Prompt Helper
Storage context

Vector Store
Graph Store
Key KG related components
KnowledgeGraphIndex is an Index to:

Indexing stage:
Extract data into KG with LLM or any other callable models
Persist KG data into storeage_context.graph_store
Querying stage:
as_query_engine() to enable 0-shot Graph RAG
as_retriever() to create an advanced Graph involving RAG
KnowledgeGraphRAGRetriever

Instanctiate:
Create a storeage_context.graph_store as the init argument.
Querying stage:
pass to RetrieverQueryEngine to become a Graph RAG query engine on any existing KG
combined with other RAG pipeline
KnowledgeGraphQueryEngine, Text2Cypher Query engine

Instanctiate:
Create a storeage_context.graph_store as the init argument.
Querying stage:
Text2cypher to get answers towards the KG in graph_store.
Optionally, generate_query() only compose a cypher query.
Preparation
Install Dependencies, prepare for contexts of Llama Index

[ ]
  1
%pip install openai ipython-ngql llama_index==0.8.9 pyvis
$ cat openrc

export OPENAI_API_KEY="sk-xxx"
export OPENAI_API_BASE="https://xxx.openai.azure.com"
export OPENAI_EMBEDDING_ENGINE="xxx"
export OPENAI_ENGINE="xxx"

export GRAPHD_HOST="1.2.3.4"
export GRAPHD_PORT="9669"
export NEBULA_USER="root"
export NEBULA_PASSWORD="nebula"
export NEBULA_ADDRESS=$GRAPHD_HOST:$GRAPHD_PORT
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28

# Read credentials

import subprocess
import os

# Define the command to source the openrc file and print environment variables
source_command = 'bash -c ". openrc && env"'

# Run the command and capture its output
completed_process = subprocess.run(source_command, shell=True, stdout=subprocess.PIPE, text=True)

# Parse the output to extract environment variables
env_output = completed_process.stdout
env_lines = env_output.splitlines()
env_variables = {}

for line in env_lines:
    key, value = line.split('=', 1)
    if any([
        "OPENAI" in key,
        "NEBULA" in key,
        "GRAPH" in key,
    ]):
        env_variables[key] = value

os.environ.update(env_variables)

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
# For OpenAI

import os

# os.environ["OPENAI_API_KEY"], handled in openrc reading

import logging
import sys

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)

from llama_index import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    KnowledgeGraphIndex,
    ServiceContext,
)

from llama_index.storage.storage_context import StorageContext
from llama_index.graph_stores import NebulaGraphStore

import logging
import sys

from IPython.display import Markdown, display


from llama_index.llms import OpenAI


# define LLM
llm = OpenAI(temperature=0, model="<model_name>")
service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)

# set global service context
set_global_service_context(service_context)
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
# For Azure OpenAI
import os
import json
import openai
from llama_index.llms import AzureOpenAI
from langchain.embeddings import OpenAIEmbeddings
from llama_index import LangchainEmbedding
from llama_index import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    KnowledgeGraphIndex,
    ServiceContext,
)

from llama_index import set_global_service_context

from llama_index.storage.storage_context import StorageContext
from llama_index.graph_stores import NebulaGraphStore
from llama_index.llms import LangChainLLM

import logging
import sys

from IPython.display import Markdown, display

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

openai.api_type = "azure"
openai.api_base = os.environ["OPENAI_API_BASE"]
openai.api_version = "2023-03-15-preview"
# os.environ["OPENAI_API_KEY"] = "INSERT OPENAI KEY"
openai.api_key = os.getenv("OPENAI_API_KEY")


lc_llm = AzureOpenAI(
    engine=os.environ["OPENAI_ENGINE"],
    temperature=0,
    model="gpt-35-turbo",
)

# You need to deploy your own embedding model as well as your own chat completion model
embedding_llm = LangchainEmbedding(
    OpenAIEmbeddings(
        model="text-embedding-ada-002",
        deployment=os.environ["OPENAI_EMBEDDING_ENGINE"],
        openai_api_key=openai.api_key,
        openai_api_base=openai.api_base,
        openai_api_type=openai.api_type,
        openai_api_version=openai.api_version,
    ),
    embed_batch_size=1,
)
service_context = ServiceContext.from_defaults(
    llm=lc_llm,
    embed_model=embedding_llm,
)

# SET Global Service Context
set_global_service_context(service_context)
account_circle
[nltk_data] Downloading package punkt to /tmp/llama_index...
[nltk_data]   Unzipping tokenizers/punkt.zip.
Create a Graph Space
💡 Do not yet have a NebulaGraph cluster? check out nebula-up, it's a one-liner built by me:

curl -fsSL nebula-up.siwei.io/install.sh | bash

[ ]
  1
  2
  3
  4
%load_ext ngql
connection_string = f"--address {os.environ['GRAPHD_HOST']} --port 9669 --user root --password {os.environ['NEBULA_PASSWORD']}"
%ngql {connection_string}
%ngql USE demo_basketballplayer
account_circle

[ ]
  1
%ngql CREATE SPACE IF NOT EXISTS rag_workshop(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);
account_circle

[ ]
  1
%ngql SHOW SPACES;
account_circle

[ ]
  1
  2
  3
  4
%%ngql
USE rag_workshop;
CREATE TAG IF NOT EXISTS entity(name string);
CREATE EDGE IF NOT EXISTS relationship(relationship string);
account_circle

[ ]
  1
%ngql CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));
account_circle

[ ]
  1
#%ngql USE rag_workshop; CLEAR SPACE rag_workshop; # clean graph space
account_circle

Storage_context with Graph_Store
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
os.environ['NEBULA_USER'] = os.environ["NEBULA_USER"]
os.environ['NEBULA_PASSWORD'] = os.environ["NEBULA_PASSWORD"]
os.environ['NEBULA_ADDRESS'] = os.environ["NEBULA_ADDRESS"]

space_name = "rag_workshop"
edge_types, rel_prop_names = ["relationship"], ["relationship"]
tags = ["entity"]

graph_store = NebulaGraphStore(
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)
storage_context = StorageContext.from_defaults(graph_store=graph_store)
🏗️ KG Building with Llama Index
Preprocess Data with data connectors
with WikipediaReader

We will download and preprecess data from: https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3

[ ]
  1
  2
  3
  4
  5
  6
  7
from llama_index import download_loader

WikipediaReader = download_loader("WikipediaReader")

loader = WikipediaReader()

documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)
Indexing Extract Triplets and Save to NebulaGraph
with KnowledgeGraphIndex

This call will take some time, it'll extract entities and relationships and store them into NebulaGraph

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    storage_context=storage_context,
    service_context=service_context,
    max_triplets_per_chunk=10,
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)
Persist storage context
[ ]
  1
  2
  3
#kg_index.storage_context.persist(persist_dir='./storage_graph')

!ls storage_graph
account_circle
docstore.json  index_store.json  vector_store.json
[ ]
  1
  2
!pwd

account_circle
/content
Restore storage_context from disk
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
from llama_index import load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)
kg_index = load_index_from_storage(
    storage_context=storage_context,
    service_context=service_context,
    max_triplets_per_chunk=10,
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
    verbose=True,
)
🧙 Text2Cypher
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
from llama_index.query_engine import KnowledgeGraphQueryEngine

from llama_index.storage.storage_context import StorageContext
from llama_index.graph_stores import NebulaGraphStore

nl2kg_query_engine = KnowledgeGraphQueryEngine(
    storage_context=storage_context,
    service_context=service_context,
    llm=lc_llm,
)
[ ]
  1
  2
  3
# activate connections
%ngql SHOW HOSTS
r = nl2kg_query_engine.query("SHOW HOSTS")
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
question = """
Tell me about Rocket?
"""

response_nl2kg = nl2kg_query_engine.query(question)

# Cypher:

print("The Cypher Query is:")

query_string = nl2kg_query_engine.generate_query(question)

display(
    Markdown(
        f"""
```cypher
{query_string}
```
"""
    )
)

%ngql {query_string}

# Answer:

print("The Answer is:")

display(Markdown(f"<b>{response_nl2kg}</b>"))
account_circle

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
question = """
What challenges do Rocket and Lylla face?
"""

response_nl2kg = nl2kg_query_engine.query(question)

# Cypher:

print("The Cypher Query is:")

query_string = nl2kg_query_engine.generate_query(question)

display(
    Markdown(
        f"""
```cypher
{query_string}
```
"""
    )
)

%ngql {query_string}

# Answer:

print("The Answer is:")

display(Markdown(f"<b>{response_nl2kg}</b>"))
account_circle

Valina LLM + Prompts doesn't work well on all questions, fine-tuning, or few-shot ways could push further.

But Graph RAG is easier as:

The query-composing doesn't rely on the higher intelligence
Easier to enable approximate starting entities
Easier to push CoT-like task-break-down in the orchestration layer
🧠 Graph RAG
KG_Index as Query Engine
[ ]
  1
  2
  3
  4
  5
kg_index_query_engine = kg_index.as_query_engine(
    retriever_mode="keyword",
    verbose=True,
    response_mode="tree_summarize",
)
[ ]
  1
  2
  3
response_graph_rag = kg_index_query_engine.query("What challenges do Rocket and Lylla face?")

display(Markdown(f"<b>{response_graph_rag}</b>"))
account_circle

[ ]
  1
  2
  3
response_graph_rag = kg_index_query_engine.query("Tell me about James Gunn.")

display(Markdown(f"<b>{response_graph_rag}</b>"))
account_circle

[ ]
  1
%ngql USE rag_workshop; MATCH p=(n)-[e:relationship*1..2]-() WHERE id(n) in ['James Gunn', 'James', 'Gunn'] RETURN p
account_circle

[ ]
  1
%ng_draw
account_circle

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
question = """
Tell me about James Gunn.
"""

response_nl2kg = nl2kg_query_engine.query(question)

# Cypher:

print("The Cypher Query is:")

query_string = nl2kg_query_engine.generate_query(question)

display(
    Markdown(
        f"""
```cypher
{query_string}
```
"""
    )
)

%ngql {query_string}

# Answer:

print("The Answer is:")

display(Markdown(f"<b>{response_nl2kg}</b>"))
account_circle

[ ]
  1
  2
  3
  4
%%ngql
MATCH p=(e1:`entity`)-[r:`relationship`]->(e2:`entity`)
WHERE e1.`entity`.`name` == 'James Gunn'
RETURN p
account_circle

[ ]
  1
%ng_draw
account_circle

See also here for comparison of text2cypher & GraphRAG

https://user-images.githubusercontent.com/1651790/260617657-102d00bc-6146-4856-a81f-f953c7254b29.mp4
https://siwei.io/en/demos/text2cypher/
While another idea is to retrieve in both ways and combine the context to fit more use cases.

Graph RAG on any existing KGs
with KnowledgeGraphRAGRetriever.

REF: https://gpt-index.readthedocs.io/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#perform-graph-rag-query

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.retrievers import KnowledgeGraphRAGRetriever

graph_rag_retriever = KnowledgeGraphRAGRetriever(
    storage_context=storage_context,
    service_context=service_context,
    llm=lc_llm,
    verbose=True,
)

query_engine = RetrieverQueryEngine.from_args(
    graph_rag_retriever, service_context=service_context
)
[ ]
  1
  2
  3
  4
response = query_engine.query(
    "Who is Rocket?",
)
display(Markdown(f"<b>{response}</b>"))
account_circle

Example of Graph RAG Chat Engine
The context mode
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
from llama_index.memory import ChatMemoryBuffer

memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

chat_engine = kg_index.as_chat_engine(
    chat_mode="context",
    memory=memory,
    verbose=True
)
response = chat_engine.chat("who is Rocket?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Lylla?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Groot?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("do they all know each other?")
display(Markdown(f"<b>{response}</b>"))
account_circle

[ ]
  1
  2
response = chat_engine.chat("But how about Lylla?")
display(Markdown(f"<b>{response}</b>"))
account_circle

Above chat_engine won't eval the "them" when doing RAG, this could be resolved with ReAct mode!

We can see, now the agent will refine the question towards RAG before the retrieval.

The ReAct mode
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

chat_engine = kg_index.as_chat_engine(
    chat_mode="react",
    memory=memory,
    verbose=True
)
response = chat_engine.chat("who is Rocket?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Lylla?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Groot?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who of them are human?")
display(Markdown(f"<b>{response}</b>"))
account_circle

Styling the chatbot, like... a rapper?
prompt was composed based on examples from Llama Index.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

prompt_as_a_rapper = """
You are a freestyle rap assistant who speaks in the fluid, rhythmic style of hip-hop. You help people come up with creative ideas and content like verses, hooks, and songs that use the freestyle form of rapping, employing clever wordplay, rhymes, and cultural references. Here are some examples of a freestyle style:

"Life's a game but it's not fair, I break the rules so I don't care."
"From the concrete who knew that a flower would grow?"
"Mic check one-two, coming through with the crew, words like a maze, catching the groove as they move."
"""

chat_engine = kg_index.as_chat_engine(
    chat_mode="context",
    memory=memory,
    verbose=True,
    system_prompt=prompt_as_a_rapper,
)
response = chat_engine.chat("who is Rocket?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Lylla?")
display(Markdown(f"<b>{response}</b>"))

response = chat_engine.chat("who is Groot?")
display(Markdown(f"<b>{response}</b>"))
account_circle

Refs:

https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/
https://github.com/wey-gu/demo-kg-build/blob/main/graph_rag_chatbot.py
https://llamaindex-chat-with-docs.streamlit.app/
[ ]
  1
  2
  3
  4
  5
from IPython.display import HTML

HTML("""
<iframe src="https://player.vimeo.com/video/857919385?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="1080" height="525" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" title="chat_graph_rag_demo"></iframe>
""")
account_circle

Graph RAG with Text2Cypher
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(
    storage_context=storage_context,
    service_context=service_context,
    llm=lc_llm,
    verbose=True,
    with_nl2graphquery=True,
)

query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(
    graph_rag_retriever_with_nl2graphquery, service_context=service_context
)
[ ]
  1
  2
  3
response = query_engine_with_nl2graphquery.query("Tell me about Rocket?")

display(Markdown(f"<b>{response}</b>"))
account_circle

Combining Graph RAG and Vector Index
REF: https://gpt-index.readthedocs.io/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html

                  ┌────┬────┬────┬────┐                  
                  │ 1  │ 2  │ 3  │ 4  │                  
                  ├────┴────┴────┴────┤                  
                  │  Docs/Knowledge   │                  
┌───────┐         │        ...        │       ┌─────────┐
│       │         ├────┬────┬────┬────┤       │         │
│       │         │ 95 │ 96 │    │    │       │         │
│       │         └────┴────┴────┴────┘       │         │
│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │
│       │                                     │         │
└───────┘  ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘
    │          ┌──────────────────────────┐        ▲     
    └──────┼──▶│  Tell me ....., please   │├───────┘     
               └──────────────────────────┘              
           │┌────┐ ┌────┐                  │             
            │ 3  │ │ 96 │ x->y, x<-z->b,..               
           │└────┘ └────┘                  │             
            ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
Vector Index creation
[ ]
  1
  2
  3
  4
vector_index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context
)
[ ]
  1
vector_rag_query_engine = vector_index.as_query_engine()
Persist and restore
[ ]
  1
#vector_index.storage_context.persist(persist_dir='./storage_vector')
[ ]
  1
  2
  3
  4
  5
  6
  7
from llama_index import load_index_from_storage

storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')
vector_index = load_index_from_storage(
    service_context=service_context,
    storage_context=storage_context_vector
)
"Cherry-picked" Examples that KG helps
Top-K Retrieval, nature of information distribution and segmentation
See more from here.

Tell me events about NASA.

VectorStore	Knowledge Graph + VectorStore	Knowledge Graph
Answer	NASA scientists report evidence for the existence of a second Kuiper Belt,
which the New Horizons spacecraft could potentially visit during the late 2020s or early 2030s.
NASA is expected to release the first study on UAP in mid-2023.
NASA's Venus probe is scheduled to be launched and to arrive on Venus in October,
partly to search for signs of life on Venus.
NASA is expected to start the Vera Rubin Observatory, the Qitai Radio Telescope,
the European Spallation Source and the Jiangmen Underground Neutrino.
NASA scientists suggest that a space sunshade could be created by mining the lunar soil and
launching it towards the Sun to form a shield against global warming.	NASA announces future space telescope programs on May 21.
NASA publishes images of debris disk on May 23. NASA discovers exoplanet LHS 475 b on May 25.
NASA scientists present evidence for the existence of a second Kuiper Belt on May 29.
NASA confirms the start of the next El Niño on June 8.
NASA produces the first X-ray of a single atom on May 31.
NASA reports the first successful beaming of solar energy from space down to a receiver on the ground on June 1.
NASA scientists report evidence that Earth may have formed in just three million years on June 14.
NASA scientists report the presence of phosphates on Enceladus, moon of the planet Saturn, on June 14.
NASA's Venus probe is scheduled to be launched and to arrive on Venus in October.
NASA's MBR Explorer is announced by the United Arab Emirates Space Agency on May 29.
NASA's Vera Rubin Observatory is expected to start in 2023.	NASA announced future space telescope programs in mid-2023,
published images of a debris disk,
and discovered an exoplanet called LHS 475 b.
Cost	1897 tokens	2046 Tokens	159 Tokens
And we could see there are indeed some knowledges added with the help of Knowledge Graph retriever:

NASA publishes images of debris disk on May 23.
NASA discovers exoplanet LHS 475 b on May 25.
The additional cost, however, does not seem to be very significant, at 7.28%: (2046-1897)/2046.

Furthermore, the answer from the knwoledge graph is extremely concise (only 159 tokens used!), but is still informative.

Takeaway: KG gets Fine-grained Segmentation of info. with the nature of interconnection/global-context-retained, it helps when retriving spread yet important knowledge pieces.

Hallucination due to w/ relationship in literal/common sense, but should not be connected in domain Knowledge
GPT-4 (WebPilot) helped me construct this question:

during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?

where, the correlation between knowledge/documents were setup in "common sence", while, they shouldn't be linked as in domain knowledge.

See this picture, they could be considered related w/o knowing they shouldn't be categorized together in the sense of e-commerce.

Insulated Greenhouse v.s. Insulated Cup

104946561_0_final 104946743_0_final
Takeaway: KG reasons things reasonably, as it holds the domain knowledge.

[ ]
  1
vector_rag_query_engine = vector_index.as_query_engine()
[ ]
  1
  2
  3
  4
  5
  6
response_vector_rag = vector_rag_query_engine.query(
"""
during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?
"""
)
display(Markdown(f"<b>{response_vector_rag}</b>"))
account_circle

[ ]
123456
response_graph_rag = kg_index_query_engine.query(
"""
during their mission on Counter-Earth, the Guardians encounter a mysterious artifact known as the 'Celestial Compass', said to be a relic from Star-Lord's Celestial lineage. Who among the Guardians was tempted to use its power for personal gain?
"""
)
display(Markdown(f"<b>{response_graph_rag}</b>"))
account_circle

[ ]
12
# backup runtime contexts
#!zip -r workshop_dump.zip openrc storage_graph storage_vector
[ ]
12
!unzip workshop_dump.zip